# References

Citations of various useful and interesting papers and articles.

## van Giffen et al 2022
Benjamin van Giffen, Dennis Herhausen, Tobias Fahse,
Overcoming the pitfalls and perils of algorithms: A classification of machine learning biases and mitigation methods,
Journal of Business Research,
Volume 144,
2022,
Pages 93-106,
ISSN 0148-2963,
https://doi.org/10.1016/j.jbusres.2022.01.076.
(https://www.sciencedirect.com/science/article/pii/S0148296322000881)
Abstract: Over the last decade, the importance of machine learning increased dramatically in business and marketing. However, when machine learning is used for decision-making, bias rooted in unrepresentative datasets, inadequate models, weak algorithm designs, or human stereotypes can lead to low performance and unfair decisions, resulting in financial, social, and reputational losses. This paper offers a systematic, interdisciplinary literature review of machine learning biases as well as methods to avoid and mitigate these biases. We identified eight distinct machine learning biases, summarized these biases in the cross-industry standard process for data mining to account for all phases of machine learning projects, and outline twenty-four mitigation methods. We further contextualize these biases in a real-world case study and illustrate adequate mitigation strategies. These insights synthesize the literature on machine learning biases in a concise manner and point to the importance of human judgment for machine learning algorithms.
Keywords: Machine learning; Artificial intelligence; Bias; Mitigation methods; Case study

## Salzburg 1997
Salzburg, Steven L
Data Mining and Knowledge Discovery, 1, 317â€“327 (1997)
On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach
http://www.cs.ru.nl/~tomh/onderwijs/lrs/lrs_files/salzberg97comparing.pdf

Abstract. An important component of many data mining projects is finding a good classification algorithm,
a process that requires very careful thought about experimental design. If not done very carefully, comparative
studies of classification and other types of algorithms can easily result in statistically invalid conclusions. This
is especially true when one is using data mining techniques to analyze very large databases, which inevitably
contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate
an experimental comparison. These phenomena and the conclusions that follow apply not only to classification,
but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative
analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions
about how to avoid the pitfalls suffered by many experimental studies.
Keywords: classification, comparative studies, statistical method
